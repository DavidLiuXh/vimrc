package run

import (
	"crypto/tls"
	"fmt"
	"io"
	"log"
	"net"
	"os"
	"runtime"
	"runtime/pprof"
	"time"

	"github.com/influxdata/influxdb/cluster"
	"github.com/influxdata/influxdb/cluster/meta11"
	"github.com/influxdata/influxdb/flux/control"
	"github.com/influxdata/influxdb/logger"
	"github.com/influxdata/influxdb/models"
	"github.com/influxdata/influxdb/monitor"
	"github.com/influxdata/influxdb/query"
	"github.com/influxdata/influxdb/services/collectd"
	"github.com/influxdata/influxdb/services/continuous_querier"
	"github.com/influxdata/influxdb/services/graphite"
	"github.com/influxdata/influxdb/services/hh"
	"github.com/influxdata/influxdb/services/httpd"
	"github.com/influxdata/influxdb/services/opentsdb"
	"github.com/influxdata/influxdb/services/placement_driver"
	"github.com/influxdata/influxdb/services/precreator"
	"github.com/influxdata/influxdb/services/rebalance"
	"github.com/influxdata/influxdb/services/retention"
	"github.com/influxdata/influxdb/services/snapshotter"
	"github.com/influxdata/influxdb/services/storage"
	"github.com/influxdata/influxdb/services/subscriber"
	"github.com/influxdata/influxdb/services/udp"
	"github.com/influxdata/influxdb/tcp"
	"github.com/influxdata/influxdb/tsdb"
	"github.com/influxdata/platform/storage/reads"
	client "github.com/influxdata/usage-client/v1"
	"go.uber.org/zap"

	// Initialize the engine package
	_ "github.com/influxdata/influxdb/tsdb/engine"
	// Initialize the index package
	_ "github.com/influxdata/influxdb/tsdb/index"
)

var startTime time.Time

func init() {
	startTime = time.Now().UTC()
}

// BuildInfo represents the build details for the server code.
type BuildInfo struct {
	Version string
	Commit  string
	Branch  string
	Time    string
}

// Server represents a container for the metadata and storage data and services.
// It is built using a Config and it manages the startup and shutdown of all
// services in the proper order.
type Server struct {
	buildInfo BuildInfo

	err     chan error
	closing chan struct{}

	BindAddress string
	Listener    net.Listener

	Logger *zap.Logger

	MetaClient *meta11.Client

	TSDBStore     *tsdb.Store
	QueryExecutor *query.Executor
	PointsWriter  *cluster.PointsWriter
	ShardWriter   *cluster.ShardWriter
	HintedHandoff *hh.Service
	Subscriber    *subscriber.Service

	Services []Service

	ClusterService *cluster.Service

	// These references are required for the tcp muxer.
	SnapshotterService *snapshotter.Service
	RebalanceService   *rebalance.Service

	Monitor *monitor.Monitor

	// Server reporting and registration
	reportingDisabled bool

	// Profiling
	CPUProfile string
	MemProfile string

	// metaUseTLS specifies if we should use a TLS connection to the meta servers
	metaUseTLS bool

	// httpAPIAddr is the host:port combination for the main HTTP API for querying and writing data
	httpAPIAddr string

	// httpUseTLS specifies if we should use a TLS connection to the http servers
	httpUseTLS bool

	// tcpAddr is the host:port combination for the TCP listener that services mux onto
	tcpAddr string

	config *Config

	PlacementDriverService *placement_driver.Service
}

// updateTLSConfig stores with into the tls config pointed at by into but only if with is not nil
// and into is nil. Think of it as setting the default value.
func updateTLSConfig(into **tls.Config, with *tls.Config) {
	if with != nil && into != nil && *into == nil {
		*into = with
	}
}

// NewServer returns a new instance of Server built from a config.
func NewServer(c *Config, buildInfo *BuildInfo, logger *zap.Logger) (*Server, error) {
	// First grab the base tls config we will use for all clients and servers
	tlsConfig, err := c.TLS.Parse()
	if err != nil {
		return nil, fmt.Errorf("tls configuration: %v", err)
	}

	// Update the TLS values on each of the configs to be the parsed one if
	// not already specified (set the default).
	updateTLSConfig(&c.HTTPD.TLS, tlsConfig)
	updateTLSConfig(&c.Subscriber.TLS, tlsConfig)
	for i := range c.OpenTSDBInputs {
		updateTLSConfig(&c.OpenTSDBInputs[i].TLS, tlsConfig)
	}

	// In 0.10.0 bind-address got moved to the top level. Check
	// The old location to keep things backwards compatible
	bind := c.BindAddress

	if !c.Data.Enabled && !c.Cluster.EnableProxy {
		return nil, fmt.Errorf("must run as either data node or proxy or both")
	}

	s := &Server{
		buildInfo: *buildInfo,
		err:       make(chan error),
		closing:   make(chan struct{}),

		BindAddress: bind,

		Logger: logger,

		MetaClient: meta11.NewClient(logger,
			c.Cluster.MetaAuthEnabled,
			c.Cluster.MetaUserName,
			c.Cluster.MetaPassword),

		reportingDisabled: c.ReportingDisabled,

		httpAPIAddr: c.HTTPD.BindAddress,
		httpUseTLS:  c.HTTPD.HTTPSEnabled,
		tcpAddr:     bind,

		config: c,
	}

	s.Monitor = monitor.New(s, c.Monitor)
	s.config.registerDiagnostics(s.Monitor)
	s.Services = append(s.Services, s.Monitor)

	if c.Data.Enabled {
		s.TSDBStore = tsdb.NewStore(c.Data.Dir)
		s.TSDBStore.EngineOptions.Config = c.Data

		// Copy TSDB configuration.
		s.TSDBStore.EngineOptions.EngineVersion = c.Data.Engine
		s.TSDBStore.EngineOptions.IndexVersion = c.Data.Index
		s.TSDBStore.EngineOptions.WALEnabled = c.Data.WALEnabled
		s.TSDBStore.EngineOptions.WALCacheCount = c.Data.WALCacheCount

		s.TSDBStore.WithLogger(s.Logger)

		// !lw! DataNode use QueryExecutor to process some requests, such as SHOW STATS
		s.QueryExecutor = query.NewExecutor()
		if s.config.Cluster.QueryLogEnabled {
			s.QueryExecutor.WithLogger(s.Logger)
		}
		s.QueryExecutor.StatementExecutor = &cluster.StatementExecutor{
			MetaClient:        s.MetaClient,
			TaskManager:       s.QueryExecutor.TaskManager,
			Monitor:           s.Monitor,
			PointsWriter:      s.PointsWriter,
			MaxSelectPointN:   c.Cluster.MaxSelectPointN,
			MaxSelectSeriesN:  c.Cluster.MaxSelectSeriesN,
			MaxSelectBucketsN: c.Cluster.MaxSelectBucketsN,
		}
		s.QueryExecutor.TaskManager.QueryTimeout = time.Duration(c.Cluster.QueryTimeout)
		s.QueryExecutor.TaskManager.LogQueriesAfter = time.Duration(c.Cluster.LogQueriesAfter)
		s.QueryExecutor.TaskManager.MaxConcurrentQueries = c.Cluster.MaxConcurrentQueries
	}

	if c.Cluster.EnableProxy {
		// Set the shard writer
		s.ShardWriter = cluster.NewShardWriter(time.Duration(c.Cluster.ShardWriterTimeout),
			c.Cluster.MaxRemoteWriteConnections)
		s.ShardWriter.MetaClient = s.MetaClient

		// Create the hinted handoff service
		s.HintedHandoff = hh.NewService(c.HintedHandoff, s.ShardWriter, s.MetaClient)
		s.HintedHandoff.WithLogger(s.Logger)
		s.HintedHandoff.Monitor = s.Monitor

		// Create the Subscriber service
		//s.Subscriber = subscriber.NewService(c.Subscriber)

		// Initialize points writer.
		s.PointsWriter = cluster.NewPointsWriter()
		s.PointsWriter.WithLogger(s.Logger)
		s.PointsWriter.WriteTimeout = time.Duration(c.Cluster.WriteTimeout)
		s.PointsWriter.TSDBStore = s.TSDBStore
		s.PointsWriter.ShardWriter = s.ShardWriter
		s.PointsWriter.HintedHandoff = s.HintedHandoff

		// Initialize meta executor.
		metaExecutor := cluster.NewMetaExecutor(s.Logger)
		metaExecutor.MetaClient = s.MetaClient

		// Initialize query executor.
		s.QueryExecutor = query.NewExecutor()
		if s.config.Cluster.QueryLogEnabled {
			s.QueryExecutor.WithLogger(s.Logger)
		}
		s.QueryExecutor.StatementExecutor = &cluster.StatementExecutor{
			MetaClient:  s.MetaClient,
			TaskManager: s.QueryExecutor.TaskManager,
			TSDBStore:   s.TSDBStore,
			ShardMapper: &cluster.LocalShardMapper{
				MetaClient: s.MetaClient,
				TSDBStore:  cluster.LocalTSDBStore{Store: s.TSDBStore},
				Timeout:    time.Duration(c.Cluster.ReadTimeout), //added by zk
				Logger:     logger.With(zap.String("service", "LocalShardMapper")),
			},
			Monitor:           s.Monitor,
			PointsWriter:      s.PointsWriter,
			MetaExecutor:      metaExecutor,
			MaxSelectPointN:   c.Cluster.MaxSelectPointN,
			MaxSelectSeriesN:  c.Cluster.MaxSelectSeriesN,
			MaxSelectBucketsN: c.Cluster.MaxSelectBucketsN,
		}
		s.QueryExecutor.TaskManager.QueryTimeout = time.Duration(c.Cluster.QueryTimeout)
		s.QueryExecutor.TaskManager.LogQueriesAfter = time.Duration(c.Cluster.LogQueriesAfter)
		s.QueryExecutor.TaskManager.MaxConcurrentQueries = c.Cluster.MaxConcurrentQueries

		// Initialize the monitor
		s.Monitor.Version = s.buildInfo.Version
		s.Monitor.Commit = s.buildInfo.Commit
		s.Monitor.Branch = s.buildInfo.Branch
		s.Monitor.BuildTime = s.buildInfo.Time
		s.Monitor.PointsWriter = (*monitorPointsWriter)(s.PointsWriter)

		s.PlacementDriverService = placement_driver.NewService(s.MetaClient)
	}
	return s, nil
}

// Statistics returns statistics for the services running in the Server.
func (s *Server) Statistics(tags map[string]string) []models.Statistic {
	var statistics []models.Statistic
	if nil != s.QueryExecutor {
		statistics = append(statistics, s.QueryExecutor.Statistics(tags)...)
	}

	if nil != s.TSDBStore {
		statistics = append(statistics, s.TSDBStore.Statistics(tags)...)
	}

	//statistics = append(statistics, s.PointsWriter.Statistics(tags)...)
	//statistics = append(statistics, s.Subscriber.Statistics(tags)...)
	for _, srv := range s.Services {
		if m, ok := srv.(monitor.Reporter); ok {
			statistics = append(statistics, m.Statistics(tags)...)
		}
	}
	return statistics
}

func (s *Server) appendSnapshotterService() {
	srv := snapshotter.NewService(s.config.Data.SnapshotPath)
	srv.TSDBStore = s.TSDBStore
	srv.MetaClient = s.MetaClient
	s.Services = append(s.Services, srv)
	s.SnapshotterService = srv
}

func (s *Server) appendRebalanceService() {
	srv := rebalance.NewService()
	srv.TSDBStore = s.TSDBStore
	srv.MetaClient = s.MetaClient
	s.Services = append(s.Services, srv)
	s.RebalanceService = srv
}

// SetLogOutput sets the logger used for all messages. It must not be called
// after the Open method has been called.
func (s *Server) SetLogOutput(w io.Writer) {
	s.Logger = logger.New(w)
}

func (s *Server) appendClusterService() {
	srv := cluster.NewService()
	srv.TSDBStore = s.TSDBStore
	srv.MetaClient = s.MetaClient
	s.Services = append(s.Services, srv)
	s.ClusterService = srv
}

func (s *Server) appendMonitorService() {
	s.Services = append(s.Services, s.Monitor)
}

func (s *Server) appendRetentionPolicyService(c retention.Config) {
	if !c.Enabled {
		return
	}
	srv := retention.NewService(c)
	srv.MetaClient = s.MetaClient
	srv.TSDBStore = s.TSDBStore
	s.Services = append(s.Services, srv)
}

func (s *Server) appendHTTPDService(c httpd.Config) {
	if !c.Enabled {
		return
	}
	srv := httpd.NewService(c, s.config.Cluster.EnableProxy)
	if s.config.Cluster.EnableProxy {
		queryLogger, _ := s.config.Logging.NewQueryLogger()
		if nil != queryLogger {
			srv.Handler.QueryLogger = queryLogger.With(zap.String("service", "query"))
			srv.Handler.SetSlowLogConfig(s.config.Cluster.LogSlow,
				s.config.Cluster.SlowlogLogSlowerThanMs)
		}
	}
	srv.Handler.MetaClient = s.MetaClient
	srv.Handler.QueryAuthorizer = meta11.NewQueryAuthorizer(s.MetaClient)
	srv.Handler.WriteAuthorizer = meta11.NewWriteAuthorizer(s.MetaClient)
	srv.Handler.QueryExecutor = s.QueryExecutor
	srv.Handler.Monitor = s.Monitor
	srv.Handler.PointsWriter = s.PointsWriter
	srv.Handler.Version = s.buildInfo.Version
	srv.Handler.BuildType = "OSS"
	ss := storage.NewStore(s.TSDBStore, s.MetaClient)
	srv.Handler.Store = ss
	srv.Handler.Controller = control.NewController(s.MetaClient, reads.NewReader(ss), s.Logger)

	s.Services = append(s.Services, srv)
}

func (s *Server) appendCollectdService(c collectd.Config) {
	if !c.Enabled {
		return
	}
	srv := collectd.NewService(c)
	//srv.MetaClient = s.MetaClient
	//srv.PointsWriter = s.PointsWriter
	s.Services = append(s.Services, srv)
}

func (s *Server) appendOpenTSDBService(c opentsdb.Config) error {
	if !c.Enabled {
		return nil
	}
	srv, err := opentsdb.NewService(c)
	if err != nil {
		return err
	}
	//srv.PointsWriter = s.PointsWriter
	//	srv.MetaClient = s.MetaClient
	s.Services = append(s.Services, srv)
	return nil
}

func (s *Server) appendGraphiteService(c graphite.Config) error {
	if !c.Enabled {
		return nil
	}
	srv, err := graphite.NewService(c)
	if err != nil {
		return err
	}

	//srv.PointsWriter = s.PointsWriter
	//srv.MetaClient = s.MetaClient
	srv.Monitor = s.Monitor
	s.Services = append(s.Services, srv)
	return nil
}

func (s *Server) appendPrecreatorService(c precreator.Config) error {
	if !c.Enabled {
		return nil
	}
	srv := precreator.NewService(c)
	srv.MetaClient = s.MetaClient
	s.Services = append(s.Services, srv)
	return nil
}

func (s *Server) appendUDPService(c udp.Config) {
	if !c.Enabled {
		return
	}
	srv := udp.NewService(c)
	//srv.PointsWriter = s.PointsWriter
	//srv.MetaClient = s.MetaClient
	s.Services = append(s.Services, srv)
}

func (s *Server) appendContinuousQueryService(c continuous_querier.Config) {
	if !c.Enabled {
		return
	}
	srv := continuous_querier.NewService(c)
	srv.MetaClient = s.MetaClient
	srv.QueryExecutor = s.QueryExecutor
	srv.Monitor = s.Monitor
	s.Services = append(s.Services, srv)
}

// Err returns an error channel that multiplexes all out of band errors received from all services.
func (s *Server) Err() <-chan error { return s.err }

// Open opens the meta and data store and all services.
func (s *Server) Open() error {
	// Start profiling, if set.
	startProfile(s.CPUProfile, s.MemProfile)

	// Open shared TCP connection.
	ln, err := net.Listen("tcp", s.BindAddress)
	if err != nil {
		return fmt.Errorf("listen: %s", err)
	}
	s.Listener = ln

	// Multiplex listener.
	mux := tcp.NewMux()
	go mux.Serve(ln)

	// initialize MetaClient.
	if err = s.initializeMetaClient(); err != nil {
		return err
	}

	// Append services.
	s.appendHTTPDService(s.config.HTTPD)

	if s.config.Data.Enabled {
		s.appendClusterService()
		s.ClusterService.Listener = mux.Listen(cluster.MuxHeader)
		// Open TSDB store.
		if err := s.TSDBStore.Open(); err != nil {
			return fmt.Errorf("open tsdb store: %s", err)
		}
		s.appendRetentionPolicyService(s.config.Retention)

		//with snapshotter service, we can get data backuped
		s.appendSnapshotterService()
		s.SnapshotterService.Listener = mux.Listen(snapshotter.MuxHeader)

		//with rebalance service, we can copy shard from one data node to another
		//and create shard on specified data node
		s.appendRebalanceService()
		s.RebalanceService.Listener = mux.Listen(rebalance.MuxHeader)
	}

	if s.config.Cluster.EnableProxy {
		// Open the hinted handoff service
		if err := s.HintedHandoff.Open(); err != nil {
			return fmt.Errorf("open hinted handoff: %s", err)
		}

		// Open the points writer service
		s.PointsWriter.MetaClient = s.MetaClient
		if err := s.PointsWriter.Open(); err != nil {
			return fmt.Errorf("open points writer: %s", err)
		}

		s.appendContinuousQueryService(s.config.ContinuousQuery)
		s.appendPrecreatorService(s.config.Precreator)

		s.Services = append(s.Services, s.PlacementDriverService)
	}

	/*
		s.PointsWriter.AddWriteSubscriber(s.Subscriber.Points())

		s.appendMonitorService()


		for _, i := range s.config.GraphiteInputs {
			if err := s.appendGraphiteService(i); err != nil {
				return err
			}
		}
		for _, i := range s.config.CollectdInputs {
			s.appendCollectdService(i)
		}
		for _, i := range s.config.OpenTSDBInputs {
			if err := s.appendOpenTSDBService(i); err != nil {
				return err
			}
		}
		for _, i := range s.config.UDPInputs {
			s.appendUDPService(i)
		}

		s.Subscriber.MetaClient = s.MetaClient
	*/

	s.Monitor.MetaClient = s.MetaClient
	s.Monitor.WithLogger(s.Logger)

	// Configure logging for all services and clients.
	for i, _ := range s.Services {
		s.Services[i].WithLogger(s.Logger)

		if err := s.Services[i].Open(); err != nil {
			return fmt.Errorf("open service: %s", err)
		}
	}

	if s.config.Data.Enabled {
		go s.MetaClient.SendHeartBeat(s.TCPAddr(),
			time.Duration(s.config.Data.HeartbeatInterval)*time.Second)

		go s.DoPendingEventsWhenStarup()
	}

	return nil
}

//execute commands send to current node when it's offline, such as Drop database
func (s *Server) DoPendingEventsWhenStarup() {
	//TODO:!LW!DropMeasurements...
	if nil != s.TSDBStore {
		localDatabases := s.TSDBStore.Databases()
		metaDatabases := s.MetaClient.Databases()

		find := false
		for i, _ := range localDatabases {
			find = false
			for j, _ := range metaDatabases {
				if metaDatabases[j].Name == localDatabases[i] {
					find = true
					break
				}
			}

			if !find {
				s.Logger.Info("DoPendingEventsWhenStarup | drop database", zap.String("db", localDatabases[i]))
				s.TSDBStore.DeleteDatabase(localDatabases[i])
			}
		}
	}
}

// Close shuts down the meta and data stores and all services.
func (s *Server) Close() error {
	stopProfile()

	// Close the listener first to stop any new connections
	if s.Listener != nil {
		s.Listener.Close()
	}

	// Close services to allow any inflight requests to complete
	// and prevent new requests from being accepted.
	for _, service := range s.Services {
		service.Close()
	}

	s.config.deregisterDiagnostics(s.Monitor)

	if s.PointsWriter != nil {
		s.PointsWriter.Close()
	}

	if s.QueryExecutor != nil {
		s.QueryExecutor.Close()
	}

	// Close the TSDBStore, no more reads or writes at this point
	if s.TSDBStore != nil {
		s.TSDBStore.Close()
	}

	if s.Subscriber != nil {
		s.Subscriber.Close()
	}

	if s.MetaClient != nil {
		s.MetaClient.Close()
	}

	close(s.closing)
	return nil
}

// reportServer reports usage statistics about the system.
func (s *Server) reportServer() {
	dbs := s.MetaClient.Databases()
	numDatabases := len(dbs)

	var (
		numMeasurements int64
		numSeries       int64
	)

	for _, db := range dbs {
		name := db.Name
		n, err := s.TSDBStore.SeriesCardinality(name)
		if err != nil {
			s.Logger.Error(fmt.Sprintf("Unable to get series cardinality for database %s: %v", name, err))
		} else {
			numSeries += n
		}

		n, err = s.TSDBStore.MeasurementsCardinality(name)
		if err != nil {
			s.Logger.Error(fmt.Sprintf("Unable to get measurement cardinality for database %s: %v", name, err))
		} else {
			numMeasurements += n
		}
	}

	clusterID := s.MetaClient.ClusterID()
	cl := client.New("")
	usage := client.Usage{
		Product: "influxdb",
		Data: []client.UsageData{
			{
				Values: client.Values{
					"os":               runtime.GOOS,
					"arch":             runtime.GOARCH,
					"version":          s.buildInfo.Version,
					"cluster_id":       fmt.Sprintf("%v", clusterID),
					"num_series":       numSeries,
					"num_measurements": numMeasurements,
					"num_databases":    numDatabases,
					"uptime":           time.Since(startTime).Seconds(),
				},
			},
		},
	}

	s.Logger.Info("Sending usage statistics to usage.influxdata.com")

	go cl.Save(usage)
}

// initializeMetaClient will set the MetaClient and join the node to the cluster if needed
func (s *Server) initializeMetaClient() error {
	// It's the first time starting up and we need to either join
	// the cluster or initialize this node as the first member
	if len(s.config.Cluster.MetaServers) == 0 {
		return fmt.Errorf("server not set to join existing cluster must run as a meta node")
	} else {
		s.MetaClient.SetMetaServers(s.config.Cluster.MetaServers)
		s.MetaClient.SetTLS(s.metaUseTLS)
	}

	if s.config.Meta.LoggingEnabled {
		s.MetaClient.WithLogger(s.Logger)
	}

	if err := s.MetaClient.Open(s.HTTPAddr(), s.TCPAddr()); err != nil {
		return err
	}

	return nil
}

// HTTPAddr returns the HTTP address used by other nodes for HTTP queries and writes.
func (s *Server) HTTPAddr() string {
	return s.remoteAddr(s.httpAPIAddr)
}

// TCPAddr returns the TCP address used by other nodes for cluster communication.
func (s *Server) TCPAddr() string {
	return s.remoteAddr(s.tcpAddr)
}

func (s *Server) remoteAddr(addr string) string {
	hostname := s.config.Hostname
	if hostname == "" {
		hostname = meta11.DefaultHostname
	}
	remote, err := meta11.DefaultHost(hostname, addr)
	if err != nil {
		return addr
	}
	return remote
}

// MetaServers returns the meta node HTTP addresses used by this server.
func (s *Server) MetaServers() []string {
	return s.MetaClient.MetaServers()
}

// Service represents a service attached to the server.
type Service interface {
	WithLogger(log *zap.Logger)
	Open() error
	Close() error
}

// prof stores the file locations of active profiles.
var prof struct {
	cpu *os.File
	mem *os.File
}

// StartProfile initializes the cpu and memory profile, if specified.
func startProfile(cpuprofile, memprofile string) {
	if cpuprofile != "" {
		f, err := os.Create(cpuprofile)
		if err != nil {
			log.Fatalf("cpuprofile: %v", err)
		}
		log.Printf("writing CPU profile to: %s\n", cpuprofile)
		prof.cpu = f
		pprof.StartCPUProfile(prof.cpu)
	}

	if memprofile != "" {
		f, err := os.Create(memprofile)
		if err != nil {
			log.Fatalf("memprofile: %v", err)
		}
		log.Printf("writing mem profile to: %s\n", memprofile)
		prof.mem = f
		runtime.MemProfileRate = 4096
	}

}

// StopProfile closes the cpu and memory profiles if they are running.
func stopProfile() {
	if prof.cpu != nil {
		pprof.StopCPUProfile()
		prof.cpu.Close()
		log.Println("CPU profile stopped")
	}
	if prof.mem != nil {
		pprof.Lookup("heap").WriteTo(prof.mem, 0)
		prof.mem.Close()
		log.Println("mem profile stopped")
	}
}

// monitorPointsWriter is a wrapper around `coordinator.PointsWriter` that helps
// to prevent a circular dependency between the `cluster` and `monitor` packages.
//type monitorPointsWriter coordinator.PointsWriter
type monitorPointsWriter cluster.PointsWriter

func (pw *monitorPointsWriter) WritePoints(database, retentionPolicy string, points models.Points) error {
	return (*cluster.PointsWriter)(pw).WritePointsPrivileged(database, retentionPolicy, models.ConsistencyLevelAny, points)
}
