package cluster

import (
	"github.com/gogo/protobuf/proto"
	"github.com/influxdata/influxdb/prometheus"
	"github.com/influxdata/influxdb/prometheus/remote"
	"github.com/influxdata/influxdb/services/storage"
	"time"

	"context"
	"encoding"
	"encoding/binary"
	"fmt"
	"io"
	"net"
	"strings"
	"sync"
	"sync/atomic"

	"github.com/influxdata/influxdb/cluster/internal"
	"github.com/influxdata/influxdb/cluster/meta11"
	"github.com/influxdata/influxdb/models"
	"github.com/influxdata/influxdb/query"
	"github.com/influxdata/influxdb/tsdb"
	"github.com/influxdata/influxql"
	"go.uber.org/zap"
)

// MaxMessageSize defines how large a message can be before we reject it
const MaxMessageSize = 1024 * 1024 * 1024 // 1GB

// MuxHeader is the header byte used in the TCP mux.
const MuxHeader = 2

/*
const (
	writeShardRequestMessage byte = iota + 1
	writeShardResponseMessage

	executeStatementRequestMessage
	executeStatementResponseMessage

	createIteratorRequestMessage
	createIteratorResponseMessage

	fieldDimensionsRequestMessage
	fieldDimensionsResponseMessage

	seriesKeysRequestMessage
	seriesKeysResponseMessage

	executeStatementRequest1Message
	executeStatementResponse1Message
)
*/

// Statistics maintained by the cluster package
const (
	writeShardReq       = "writeShardReq"
	writeShardPointsReq = "writeShardPointsReq"
	writeShardFail      = "writeShardFail"

	createIteratorReq  = "createIteratorReq"
	createIteratorResp = "createIteratorResp"

	fieldDimensionsReq  = "fieldDimensionsReq"
	fieldDimensionsResp = "fieldDimensionsResp"

	fieldTypeReq  = "fieldTypeReq"
	fieldTypeResp = "fieldTypeResp"
)

type Statistics struct {
	WriteShardReq       int64
	WriteShardPointsReq int64
	WriteShardFail      int64

	CreateIteratorReq  int64
	CreateIteratorResp int64
}

func (s *Service) Statistics(tags map[string]string) []models.Statistic {
	statistics := []models.Statistic{{
		Name: "cluster",
		Tags: tags,
		Values: map[string]interface{}{
			writeShardReq:       atomic.LoadInt64(&s.statMap.WriteShardReq),
			writeShardPointsReq: atomic.LoadInt64(&s.statMap.WriteShardPointsReq),
			writeShardFail:      atomic.LoadInt64(&s.statMap.WriteShardFail),

			createIteratorReq:  atomic.LoadInt64(&s.statMap.CreateIteratorReq),
			createIteratorResp: atomic.LoadInt64(&s.statMap.CreateIteratorResp),
		},
	}}

	return statistics
}

// Service processes data received over raw TCP connections.
type Service struct {
	mu sync.RWMutex

	wg      sync.WaitGroup
	closing chan struct{}

	Listener net.Listener

	MetaClient interface {
		ShardOwner(shardID uint64) (string, string, *meta11.ShardGroupInfo)
		Database(name string) *meta11.DatabaseInfo
		ShardGroupsByTimeRange(database, policy string, min, max time.Time) (a []meta11.ShardGroupInfo, err error)
	}

	TSDBStore *tsdb.Store
	ss        *storage.Store //added by zk

	Logger  *zap.Logger
	statMap *Statistics
}

// NewService returns a new instance of Service.
func NewService() *Service {
	return &Service{
		closing: make(chan struct{}),
		Logger:  zap.NewNop(),
		statMap: &Statistics{},
	}
}

// Open opens the network listener and begins serving requests.
func (s *Service) Open() error {
	s.ss = storage.NewStore(s.TSDBStore, s.MetaClient) //added by zk

	s.Logger.Info("Starting cluster service")
	// Begin serving conections.
	s.wg.Add(1)
	go s.serve()

	return nil
}

// SetLogger sets the internal logger to the logger passed in.
func (s *Service) WithLogger(log *zap.Logger) {
	s.Logger = log.With(zap.String("service", "cluster"))
}

// serve accepts connections from the listener and handles them.
func (s *Service) serve() {
	defer s.wg.Done()

	for {
		// Check if the service is shutting down.
		select {
		case <-s.closing:
			return
		default:
		}

		// Accept the next connection.
		conn, err := s.Listener.Accept()
		if err != nil {
			if strings.Contains(err.Error(), "connection closed") {
				s.Logger.Error("cluster service accept error", zap.Error(err))
				return
			}
			s.Logger.Error("accept error", zap.Error(err))
			continue
		}

		// Delegate connection handling to a separate goroutine.
		s.wg.Add(1)
		go func() {
			defer s.wg.Done()
			s.handleConn(conn)
		}()
	}
}

// Close shuts down the listener and waits for all connections to finish.
func (s *Service) Close() error {
	if s.Listener != nil {
		s.Listener.Close()
	}

	// Shut down all handlers.
	close(s.closing)
	s.wg.Wait()

	return nil
}

// handleConn services an individual TCP connection.
func (s *Service) handleConn(conn net.Conn) {
	// Ensure connection is closed when service is closed.
	closing := make(chan struct{})
	defer close(closing)
	go func() {
		select {
		case <-closing:
		case <-s.closing:
		}
		conn.Close()
	}()

	s.Logger.Info("accept remote connection from ", zap.String("addr", conn.RemoteAddr().String()))
	defer func() {
		s.Logger.Info("close remote connection from ", zap.String("addr", conn.RemoteAddr().String()))
	}()
	for {
		// Read type-length-value.
		typ, err := ReadType(conn)
		if err != nil {
			if strings.HasSuffix(err.Error(), "EOF") {
				return
			}
			s.Logger.Error("unable to read type ", zap.Error(err))
			return
		}

		// Delegate message processing by type.
		switch typ {
		case writeShardRequestMessage:
			buf, err := ReadLV(conn)
			if err != nil {
				s.Logger.Error("unable to read length-value:", zap.Error(err))
				return
			}

			atomic.AddInt64(&s.statMap.WriteShardReq, 1)
			err = s.processWriteShardRequest(buf)
			if err != nil {
				s.Logger.Error("process write shard error:", zap.Error(err))
			}
			s.writeShardResponse(conn, err)
		case executeStatementRequestMessage:
			buf, err := ReadLV(conn)
			if err != nil {
				s.Logger.Error("unable to read length-value:", zap.Error(err))
				return
			}

			//Drop Statement
			err = s.processExecuteStatementRequest(buf)
			if err != nil {
				s.Logger.Error("process execute statement error:", zap.Error(err))
			}
			s.writeShardResponse(conn, err)
		case executeStatement1RequestMessage:
			//Drop Statement
			s.processExecuteStatement1Request(conn)
			//if err != nil {
			//	s.Logger.Error("process execute statement error:", zap.Error(err))
			//}
			//s.writeShardResponse(conn, err)
			//return
		case promReadRequestMessage:
			s.processPromReadRequest(conn)
		case createIteratorRequestMessage:
			atomic.AddInt64(&s.statMap.CreateIteratorReq, 1)
			s.processCreateIteratorRequest(conn)
			return
		case fieldDimensionsRequestMessage:
			//s.statMap.Add(fieldDimensionsReq, 1)
			s.processFieldDimensionsRequest(conn)
			return
		case fieldTypeRequestMessage:
			//s.statMap.Add(fieldTypeReq, 1)
			s.processFieldTypeRequest(conn)
			return
		default:
			s.Logger.Error("cluster service message type not found:", zap.Int("type", int(typ)))
		}
	}
}

func (s *Service) processExecuteStatementRequest(buf []byte) error {
	// Unmarshal the request.
	var req ExecuteStatementRequest
	if err := req.UnmarshalBinary(buf); err != nil {
		return err
	}

	// Parse the InfluxQL statement.
	stmt, err := influxql.ParseStatement(req.Statement())
	if err != nil {
		return err
	}

	return s.executeStatement(stmt, req.Database())
}

func (s *Service) executeStatement(stmt influxql.Statement, database string) error {
	switch t := stmt.(type) {
	case *influxql.DropDatabaseStatement:
		return s.TSDBStore.DeleteDatabase(t.Name)
	case *influxql.DropMeasurementStatement:
		return s.TSDBStore.DeleteMeasurement(database, t.Name)
	case *influxql.DropSeriesStatement:
		return s.TSDBStore.DeleteSeries(database, t.Sources, t.Condition)
	case *influxql.DropRetentionPolicyStatement:
		return s.TSDBStore.DeleteRetentionPolicy(database, t.Name)
	case *influxql.DeleteSeriesStatement:
		return s.TSDBStore.DeleteSeries(database, t.Sources, t.Condition)
	default:
		return fmt.Errorf("%q should not be executed across a cluster", stmt.String())
	}
}

func (s *Service) processExecuteStatement1Request(conn net.Conn) {
	buf, err := ReadLV(conn)
	if err != nil {
		resp := &ExecuteStatement1Response{}
		if err := EncodeTLV(conn, executeStatement1ResponseMessage, resp); err != nil {
			s.Logger.Error("error writing executeStatement1ResponseMessage response: %s", zap.Error(err))
		}
		s.Logger.Error("unable to read length-value:", zap.Error(err))
		return
	}

	// Unmarshal the request.
	var req ExecuteStatement1Request
	if err := req.UnmarshalBinary(buf); err != nil {
		resp := &ExecuteStatement1Response{}
		if err := EncodeTLV(conn, executeStatement1ResponseMessage, resp); err != nil {
			s.Logger.Error("error writing executeStatement1ResponseMessage response: %s", zap.Error(err))
		}
		return
	}

	// Parse the InfluxQL statement.
	stmt, err := influxql.ParseStatement(req.Statement())
	if err != nil {
		resp := &ExecuteStatement1Response{}
		if err := EncodeTLV(conn, executeStatement1ResponseMessage, resp); err != nil {
			s.Logger.Error("error writing executeStatement1ResponseMessage response: %s", zap.Error(err))
		}
		return
	}

	//shardIDs := req.ShardIDs()

	var mms []*internal.MTV
	switch t := stmt.(type) {
	case *influxql.ShowMeasurementsStatement:
		names, _ := s.TSDBStore.MeasurementNames(query.OpenAuthorizer, t.Database, t.Condition)
		mms = make([]*internal.MTV, 0, len(names))
		for _, n := range names {
			tmp := string(n)
			mm := internal.MTV{Name: &tmp}
			mms = append(mms, &mm)
		}
	case *influxql.ShowTagKeysStatement:
		//valuer := &influxql.NowValuer{Now: time.Now()}
		valuer := &influxql.NowValuer{}
		cond, _, _ := influxql.ConditionExpr(t.Condition, valuer)
		names, _ := s.TSDBStore.TagKeys(query.OpenAuthorizer, req.ShardIDs(), cond)
		mms = make([]*internal.MTV, 0, len(names))
		for i, n := range names {
			mm := internal.MTV{Name: &names[i].Measurement}
			mm.Tags = make([]*internal.TV, 0, len(n.Keys))
			for i, _ := range n.Keys {
				tv := internal.TV{K: &n.Keys[i]}
				mm.Tags = append(mm.Tags, &tv)
			}
			mms = append(mms, &mm)
		}
	case *influxql.ShowTagValuesStatement:
		valuer := &influxql.NowValuer{}
		cond, _, _ := influxql.ConditionExpr(t.Condition, valuer)
		names, _ := s.TSDBStore.TagValues(query.OpenAuthorizer, req.ShardIDs(), cond)
		mms = make([]*internal.MTV, 0, len(names))
		for i, n := range names {
			mm := internal.MTV{Name: &names[i].Measurement}
			mm.Tags = make([]*internal.TV, 0, len(n.Values))
			for i, _ := range n.Values {
				tv := internal.TV{
					K: &n.Values[i].Key,
					V: &n.Values[i].Value,
				}
				mm.Tags = append(mm.Tags, &tv)
			}
			mms = append(mms, &mm)
		}

	default:
		fmt.Errorf("%q should not be executed across a cluster", stmt.String())
	}

	resp := &ExecuteStatement1Response{}
	resp.SetMeasurements(mms)

	if err := EncodeTLV(conn, executeStatement1ResponseMessage, resp); err != nil {
		s.Logger.Error("error writing executeStatement1ResponseMessage response: %s", zap.Error(err))
	}
	return
}

func (s *Service) processPromReadRequest(conn net.Conn) {
	buf, err := ReadLV(conn)
	if err != nil {
		resp := &remote.ReadResponseWrap{}
		data, err := proto.Marshal(resp)
		if err = WriteTLV(conn, promReadResponseMessage, data); err != nil {
			s.Logger.Error("error writing promReadResponseMessage response: %s", zap.Error(err))
		}
		s.Logger.Error("unable to read length-value:", zap.Error(err))
		return
	}

	// Unmarshal the request.
	var req remote.ReadRequestWrap
	if err := proto.Unmarshal(buf, &req); err != nil {
		resp := &remote.ReadResponseWrap{}
		data, err := proto.Marshal(resp)
		if err = WriteTLV(conn, promReadResponseMessage, data); err != nil {
			s.Logger.Error("error writing promReadResponseMessage response: %s", zap.Error(err))
		}
		return
	}

	readRequest, err := prometheus.ReadRequestToInfluxStorageRequest(req.Request, req.Db, req.Rp)
	if err != nil {
		resp := &remote.ReadResponseWrap{}
		data, err := proto.Marshal(resp)
		if err = WriteTLV(conn, promReadResponseMessage, data); err != nil {
			s.Logger.Error("error writing promReadResponseMessage response: %s", zap.Error(err))
		}
		return
	}

	ctx := context.Background()
	rs, err := s.ss.Read(ctx, readRequest)
	if err != nil {
		resp := &remote.ReadResponseWrap{}
		data, err := proto.Marshal(resp)
		if err = WriteTLV(conn, promReadResponseMessage, data); err != nil {
			s.Logger.Error("error writing promReadResponseMessage response: %s", zap.Error(err))
		}
		return
	}
	if rs == nil {
		resp := &remote.ReadResponseWrap{
			Serieskey:  [][]byte{{}},
			Timeseries: []*remote.TimeSeries{{}},
		}
		data, _ := proto.Marshal(resp)
		if err := WriteTLV(conn, promReadResponseMessage, data); err != nil {
			s.Logger.Error("error writing promReadResponseMessage response: %s", zap.Error(err))
		}
		return
	}

	defer rs.Close()

	resp := &remote.ReadResponseWrap{}
	for rs.Next() { //一个循环对应一个series+fieldkey
		cur := rs.Cursor()
		if cur == nil {
			// no data for series key + field combination
			continue
		}

		tags := prometheus.RemoveInfluxSystemTags(rs.Tags())
		var unsupportedCursor string
		switch cur := cur.(type) {
		case tsdb.FloatArrayCursor: //对应的类floatArrayAscendingCursor
			var series *remote.TimeSeries
			for {
				a := cur.Next() //一次读出一大块
				if a.Len() == 0 {
					break
				}

				// We have some data for this series.
				if series == nil {
					series = &remote.TimeSeries{
						Labels: prometheus.ModelTagsToLabelPairs(tags),
					}
				}

				for i, ts := range a.Timestamps {
					series.Samples = append(series.Samples, &remote.Sample{
						TimestampMs: ts / int64(time.Millisecond),
						Value:       a.Values[i],
					})
				}
			}
			serieskey := tsdb.AppendSeriesKey(nil, rs.Name(), rs.SeriesTags())
			// There was data for the series.
			if series != nil {
				resp.Serieskey = append(resp.Serieskey, serieskey)
				resp.Timeseries = append(resp.Timeseries, series)
			}
		case tsdb.IntegerArrayCursor:
			unsupportedCursor = "int64"
		case tsdb.UnsignedArrayCursor:
			unsupportedCursor = "uint"
		case tsdb.BooleanArrayCursor:
			unsupportedCursor = "bool"
		case tsdb.StringArrayCursor:
			unsupportedCursor = "string"
		default:
			panic(fmt.Sprintf("unreachable: %T", cur))
		}
		cur.Close()

		if len(unsupportedCursor) > 0 {
			s.Logger.Info("Prometheus can't read cursor",
				zap.String("cursor_type", unsupportedCursor),
				zap.Stringer("series", tags),
			)
		}
	}
	data, err := proto.Marshal(resp)
	if err := WriteTLV(conn, promReadResponseMessage, data); err != nil {
		s.Logger.Error("error writing promReadResponseMessage response: %s", zap.Error(err))
	}
	return
}

func (s *Service) processWriteShardRequest(buf []byte) error {
	// Build request
	var req WriteShardRequest
	if err := req.UnmarshalBinary(buf); err != nil {
		return err
	}

	points := req.Points()
	atomic.AddInt64(&s.statMap.WriteShardPointsReq, int64(len(points)))
	err := s.TSDBStore.WriteToShard(req.ShardID(), points)

	// We may have received a write for a shard that we don't have locally because the
	// sending node may have just created the shard (via the metastore) and the write
	// arrived before the local store could create the shard.  In this case, we need
	// to check the metastore to determine what database and retention policy this
	// shard should reside within.
	if err == tsdb.ErrShardNotFound {
		db, rp := req.Database(), req.RetentionPolicy()
		if db == "" || rp == "" {
			s.Logger.Error("drop write request: shard. no database or rentention policy received", zap.Uint64("shard", req.ShardID()))
			return nil
		}

		//UPDATE_CODE
		//err = s.TSDBStore.CreateShard(req.Database(), req.RetentionPolicy(), req.ShardID())
		//in the 1.7 version, "true" means enable read&write
		err = s.TSDBStore.CreateShard(req.Database(), req.RetentionPolicy(), req.ShardID(), true)
		if err != nil {
			atomic.AddInt64(&s.statMap.WriteShardFail, 1)
			return fmt.Errorf("create shard %d: %s", req.ShardID(), err)
		}

		err = s.TSDBStore.WriteToShard(req.ShardID(), points)
		if err != nil {
			atomic.AddInt64(&s.statMap.WriteShardFail, 1)
			return fmt.Errorf("write shard %d: %s", req.ShardID(), err)
		}
	}

	if err != nil {
		atomic.AddInt64(&s.statMap.WriteShardFail, 1)
		return fmt.Errorf("write shard %d: %s", req.ShardID(), err)
	}

	return nil
}

func (s *Service) writeShardResponse(w io.Writer, e error) {
	// Build response.
	var resp WriteShardResponse
	if e != nil {
		resp.SetCode(1)
		resp.SetMessage(e.Error())
	} else {
		resp.SetCode(0)
	}

	// Marshal response to binary.
	buf, err := resp.MarshalBinary()
	if err != nil {
		s.Logger.Error("error marshalling shard response:", zap.Error(err))
		return
	}

	// Write to connection.
	if err := WriteTLV(w, writeShardResponseMessage, buf); err != nil {
		s.Logger.Error("write shard response error:", zap.Error(err))
	}
}

func (s *Service) processCreateIteratorRequest(conn net.Conn) {
	defer conn.Close()
	ctx, cancel := context.WithCancel(context.Background())
	var itrs []query.Iterator
	var itr query.Iterator
	defer func() {
		if itr != nil {
			itr.Close()
		}
	}()
	if err := func() error {
		// Parse request.
		var req CreateIteratorRequest
		err := DecodeLV(conn, &req)
		if err != nil {
			return err
		}
		req.Opt.InterruptCh = ctx.Done()
		ic := s.TSDBStore.ShardGroup(req.ShardIDs)
		//itr, err = sg.CreateIterator(ctx, measurement, IteratorOptions)
		itrs = make([]query.Iterator, 0, len(req.Opt.Sources))
		for _, source := range req.Opt.Sources {
			switch source := source.(type) {
			case *influxql.Measurement:
				if source.Regex != nil {
					measurements := ic.MeasurementsByRegex(source.Regex.Val)
					inputs := make([]query.Iterator, 0, len(measurements))
					for _, measurement := range measurements {
						mm := source.Clone()
						mm.Name = measurement
						i, err := ic.CreateIterator(ctx, mm, req.Opt)
						if err != nil {
							query.Iterators(inputs).Close()
							return err
						}
						inputs = append(inputs, i)
					}
					itr, err := query.Iterators(inputs).Merge(req.Opt)
					if err != nil {
						query.Iterators(inputs).Close()
						return err
					}
					itrs = append(itrs, itr)
				} else {
					itr, err := ic.CreateIterator(ctx, source, req.Opt)
					if err != nil {
						return err
					}
					itrs = append(itrs, itr)
				}

			case *influxql.SubQuery:
				continue
			}
		}
		itr, err = query.Iterators(itrs).Merge(req.Opt)
		return err
	}(); err != nil {
		query.Iterators(itrs).Close()
		s.Logger.Error("error reading CreateIterator request:", zap.Error(err))
		EncodeTLV(conn, createIteratorResponseMessage, &CreateIteratorResponse{Err: err})
		return
	}

	// Encode success response.
	if err := EncodeTLV(conn, createIteratorResponseMessage, &CreateIteratorResponse{}); err != nil {
		s.Logger.Error("error writing CreateIterator response:", zap.Error(err))
		return
	}

	// Exit if no iterator was produced.
	if itr == nil {
		s.Logger.Error("error writing itr is nil")
		return
	}

	// Stream iterator to connection.

	if err := query.NewIteratorEncoder(conn).EncodeIterator(itr); err != nil {
		cancel()
		itr.Close()
		s.Logger.Error("error encoding CreateIterator iterator:", zap.Error(err))
	}

	return
}

func (s *Service) processFieldDimensionsRequest(conn net.Conn) {
	mapping := make(map[influxql.Measurement]fd)
	var fd fd
	if err := func() error {
		// Parse request.
		var req FieldDimensionsRequest
		err := DecodeLV(conn, &req)
		if err != nil {
			return err
		}

		ic := s.TSDBStore.ShardGroup(req.ShardIDs)
		for _, source := range req.Sources {
			switch source := source.(type) {
			case *influxql.Measurement:
				var measurements []string
				if source.Regex != nil {
					measurements = ic.MeasurementsByRegex(source.Regex.Val)
				} else {
					measurements = []string{source.Name}
				}

				fd.F, fd.D, err = ic.FieldDimensions(measurements)
				mapping[*source] = fd
				if err != nil {
					return err
				}

			case *influxql.SubQuery:
				continue
			}
		}
		// Generate a single iterator from all shards.
		return nil
	}(); err != nil {
		s.Logger.Error("error reading FieldDimensions request: %s", zap.Error(err))
		EncodeTLV(conn, fieldDimensionsResponseMessage, &FieldDimensionsResponse{Err: err})
		return
	}

	// Encode success response.
	if err := EncodeTLV(conn, fieldDimensionsResponseMessage, &FieldDimensionsResponse{
		Mapping: mapping,
		Err:     nil,
	}); err != nil {
		s.Logger.Error("error writing FieldDimensions response: %s", zap.Error(err))
		return
	}
}

func (s *Service) processFieldTypeRequest(conn net.Conn) {
	var req FieldTypeRequest
	if err := func() error {
		// Parse request.
		err := DecodeLV(conn, &req)
		if err != nil {
			return err
		}

		ic := s.TSDBStore.ShardGroup(req.ShardIDs)
		for mm, fd := range req.Mapping {
			var measurements []string
			if mm.Regex != nil {
				measurements = ic.MeasurementsByRegex(mm.Regex.Val)
			} else {
				measurements = []string{mm.Name}
			}

			for f, _ := range fd.F {
				var typ influxql.DataType
				for _, name := range measurements {
					t := ic.MapType(name, f)
					if typ.LessThan(t) {
						typ = t
					}
				}
				req.Mapping[mm].F[f] = typ
			}

		}
		// Generate a single iterator from all shards.
		return nil
	}(); err != nil {
		s.Logger.Error("error reading FieldDimensions request: %s", zap.Error(err))
		EncodeTLV(conn, fieldTypeResponseMessage, &FieldTypeResponse{Err: err})
		return
	}

	//for mm, fd := range req.Mapping {
	//	fmt.Print(mm)
	//	fmt.Printf("\n")
	//	for f, t := range fd.F {
	//		fmt.Print(f, t)
	//		fmt.Printf("\n")
	//	}
	//	for d, _ := range fd.D {
	//		fmt.Print(d)
	//		fmt.Printf("\n")
	//	}
	//}
	//fmt.Printf("end\n")

	// Encode success response.
	if err := EncodeTLV(conn, fieldTypeResponseMessage, &FieldTypeResponse{
		Mapping: req.Mapping,
		Err:     nil,
	}); err != nil {
		s.Logger.Error("error writing FieldDimensions response: %s", zap.Error(err))
		return
	}
}

/*
func (s *Service) processSeriesKeysRequest(conn net.Conn) {
	var seriesList influxql.SeriesList
	if err := func() error {
		// Parse request.
		var req SeriesKeysRequest
		if err := DecodeLV(conn, &req); err != nil {
			return err
		}

		// Collect iterator creators for each shard.
		ics := make([]influxql.IteratorCreator, 0, len(req.ShardIDs))
		for _, shardID := range req.ShardIDs {
			ic := s.TSDBStore.ShardIteratorCreator(shardID)
			if ic == nil {
				return nil
			}
			ics = append(ics, ic)
		}

		// Generate a single iterator from all shards.
		a, err := influxql.IteratorCreators(ics).SeriesKeys(req.Opt)
		if err != nil {
			return err
		}
		seriesList = a

		return nil
	}(); err != nil {
		s.Logger.Printf("error reading SeriesKeys request: %s", err)
		EncodeTLV(conn, seriesKeysResponseMessage, &SeriesKeysResponse{Err: err})
		return
	}

	// Encode success response.
	if err := EncodeTLV(conn, seriesKeysResponseMessage, &SeriesKeysResponse{
		SeriesList: seriesList,
	}); err != nil {
		s.Logger.Printf("error writing SeriesKeys response: %s", err)
		return
	}
}

*/

// ReadTLV reads a type-length-value record from r.
func ReadTLV(r io.Reader) (byte, []byte, error) {
	typ, err := ReadType(r)
	if err != nil {
		return 0, nil, err
	}

	buf, err := ReadLV(r)
	if err != nil {
		return 0, nil, err
	}
	return typ, buf, err
}

// ReadType reads the type from a TLV record.
func ReadType(r io.Reader) (byte, error) {
	var typ [1]byte
	if _, err := io.ReadFull(r, typ[:]); err != nil {
		return 0, fmt.Errorf("read message type: %s", err)
	}
	return typ[0], nil
}

// ReadLV reads the length-value from a TLV record.
func ReadLV(r io.Reader) ([]byte, error) {
	// Read the size of the message.
	var sz int64
	if err := binary.Read(r, binary.BigEndian, &sz); err != nil {
		return nil, fmt.Errorf("read message size: %s", err)
	}

	if sz >= MaxMessageSize {
		return nil, fmt.Errorf("max message size of %d exceeded: %d", MaxMessageSize, sz)
	}

	// Read the value.
	buf := make([]byte, sz)
	if _, err := io.ReadFull(r, buf); err != nil {
		return nil, fmt.Errorf("read message value: %s", err)
	}

	return buf, nil
}

// WriteTLV writes a type-length-value record to w.
func WriteTLV(w io.Writer, typ byte, buf []byte) error {
	if err := WriteType(w, typ); err != nil {
		return err
	}
	if err := WriteLV(w, buf); err != nil {
		return err
	}
	return nil
}

// WriteType writes the type in a TLV record to w.
func WriteType(w io.Writer, typ byte) error {
	if _, err := w.Write([]byte{typ}); err != nil {
		return fmt.Errorf("write message type: %s", err)
	}
	return nil
}

// WriteLV writes the length-value in a TLV record to w.
func WriteLV(w io.Writer, buf []byte) error {
	// Write the size of the message.
	if err := binary.Write(w, binary.BigEndian, int64(len(buf))); err != nil {
		return fmt.Errorf("write message size: %s", err)
	}

	// Write the value.
	if _, err := w.Write(buf); err != nil {
		return fmt.Errorf("write message value: %s", err)
	}
	return nil
}

// EncodeTLV encodes v to a binary format and writes the record-length-value record to w.
func EncodeTLV(w io.Writer, typ byte, v encoding.BinaryMarshaler) error {
	if err := WriteType(w, typ); err != nil {
		return err
	}
	if err := EncodeLV(w, v); err != nil {
		return err
	}
	return nil
}

// EncodeLV encodes v to a binary format and writes the length-value record to w.
func EncodeLV(w io.Writer, v encoding.BinaryMarshaler) error {
	buf, err := v.MarshalBinary()
	if err != nil {
		return err
	}

	if err := WriteLV(w, buf); err != nil {
		return err
	}
	return nil
}

// DecodeTLV reads the type-length-value record from r and unmarshals it into v.
func DecodeTLV(r io.Reader, v encoding.BinaryUnmarshaler) (typ byte, err error) {
	typ, err = ReadType(r)
	if err != nil {
		return 0, err
	}
	if err := DecodeLV(r, v); err != nil {
		return 0, err
	}
	return typ, nil
}

// DecodeLV reads the length-value record from r and unmarshals it into v.
func DecodeLV(r io.Reader, v encoding.BinaryUnmarshaler) error {
	buf, err := ReadLV(r)
	if err != nil {
		return err
	}

	if err := v.UnmarshalBinary(buf); err != nil {
		return err
	}
	return nil
}
