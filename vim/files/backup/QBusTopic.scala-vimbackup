/**
 * Copyright 2015 Yahoo Inc. Licensed under the Apache License, Version 2.0
 * See accompanying LICENSE file.
 */

package controllers

import java.util.Properties

import features.ApplicationFeatures
import models.navigation.Menus
import play.api.i18n.{MessagesApi, I18nSupport}
import play.api.mvc._
import play.api.Logger
import play.api.libs.json._
import play.api.Configuration

import scala.concurrent.{ExecutionContext, Future, Await}
import scala.concurrent.duration._
import scala.util.{Failure, Success, Try}
import scala.util.control.Breaks._

import scalaz.{-\/, \/, \/-}

import kafka.manager._
import kafka.manager.model._
import kafka.manager.model.ActorModel._

import com.typesafe.config.{ConfigFactory, Config}

import java.nio.charset.StandardCharsets

import play.api.libs.ws._
import play.api.libs.ws.ning._
import play.api.libs.json._

import org.apache.curator.framework._
import org.apache.curator.retry.BoundedExponentialBackoffRetry
import org.apache.curator.framework.recipes.cache._

import org.apache.zookeeper.CreateMode
import org.apache.zookeeper.data.Stat
import org.apache.zookeeper.KeeperException.{NodeExistsException, NoNodeException}

import scala.collection.JavaConversions._
import scala.collection.concurrent.TrieMap
import scala.collection.mutable
/**
 * @author hiral
 */
object QBusTopic {
  val QBusTopicRetentionMs = "kafka-manager.qbus-topic-retention-ms"

  val DefaultConfig: Config = {
    val defaults: Map[String, _ <: AnyRef] = Map(
      QBusTopicRetentionMs -> "172800000"
    )
    import scala.collection.JavaConverters._
    ConfigFactory.parseMap(defaults.asJava)
  }
}

import QBusTopic._
class QBusTopic(val messagesApi: MessagesApi, val kafkaManagerContext: KafkaManagerContext, configuration: Configuration)
                  (implicit af: ApplicationFeatures, menus: Menus) extends Controller with I18nSupport {

  val logger: Logger = Logger("kafka.manager")
  val config: Config = configuration.underlying
  private[this] val configWithDefaults = config.withFallback(DefaultConfig)

  import play.api.libs.concurrent.Execution.Implicits.defaultContext

  private[this] val kafkaManager = kafkaManagerContext.getKafkaManager

  def getTopicDetail(clustername:String, topic:String) = Action.async { request =>
    val futureErrorOrTopicIdentity = kafkaManager.getTopicIdentity(clustername, topic)
    val futureErrorOrConsumerList = kafkaManager.getConsumersForTopic(clustername, topic)

    futureErrorOrTopicIdentity.zip(futureErrorOrConsumerList).map {
      case (errorOrTopicIdentity, errorOrConsumerList) =>
        var result:String = ""
        var errorMsg:String = ""
        
        var topicInfo:TopicIdentity = null
        var consumers:ConsumerListExtended = null
        errorOrTopicIdentity match {
          case -\/(e) => errorMsg = e.msg 
          case \/-(t) => topicInfo = t
        }

        if (errorMsg != "") {
          result = "{\"errcode\":\"Failed\",\"errmsg\":\"" + errorMsg + "\"}"
        } else {
          var retention_ms = "";

          var retention_list = topicInfo.config.filter(item => item._1 == "retention.ms").map(item => item._2)
          if (retention_list.isEmpty()) {
            retention_ms = configWithDefaults.getString(QBusTopicRetentionMs)
          } else {
            retention_ms = retention_list.head
          }

          result = "{\"errcode\":\"OK\",\"errmsg\":\"\", \"result\":{"
          result += "\"replications\":" + topicInfo.replicationFactor + ","
          result += "\"partitons\":" + topicInfo.partitions + ","
          result += "\"sum_offset\":" + topicInfo.summedTopicOffsets + ","
          result += "\"retention_ms\":" + retention_ms + ","
          result += "\"brokers\": ["

          for (i <- 0 until topicInfo.partitionsByBroker.size) {
            if (i != 0) {
              result += ","
            }

            result += "{"
            result += "\"id\":" + topicInfo.partitionsByBroker(i).id + ","

            result += "\"partitions\":\"" 
            for (j <- 0 until topicInfo.partitionsByBroker(i).partitions.size) {
              if (j != 0) {
                result += ","
              }
              result += topicInfo.partitionsByBroker(i).partitions(j)
            }
            result += "\""
            
            result += "}"
          }
          result += "],"

          var k = 0
          result += "\"consumer_groups\":["
          errorOrConsumerList.foreach (consumerList =>
              for ((c:String, ct: ConsumerType) <- consumerList) {
                if (k != 0) {
                  result += ","
                }
                k += 1
                result += "{\"name\":\"" + c + "\",\"type\":\"" + ct + "\"}"
              })
          result += "]"

          result += "}"
          result += "}"
        }

        Ok(result)
    }
  }

  def addTopic() = Action.async(parse.tolerantJson) { request =>
    var config = new Properties()
    Try(config.setProperty("retention.ms", ((request.body \ "retention").as[Int] * 3600 * 1000).toString))
    Try(config.setProperty("max.message.bytes", ((request.body \ "max_message").as[Long]).toString))
    kafkaManager.createTopic((request.body \ "cluster").as[String],
      (request.body \ "topic").as[String],
      (request.body \ "partitions").as[Int],
      (request.body \ "replications").as[Int],
      config).map {
        case -\/(e) => Ok("{\"errcode\":\"Failed\", \"errmsg\":\"" + e.msg + "\"}") 
        case \/-(ok) => Ok("{\"errcode\":\"Ok\", \"errmsg\":\"\"}")
      }
  }

  def updateTopicConfig() = Action.async(parse.tolerantJson) {request =>
    kafkaManager.getTopicIdentity((request.body \ "cluster").as[String],
      (request.body \ "topic").as[String]).flatMap {
        case -\/(e) => Future(Ok("{\"errcode\":\"Failed\", \"errmsg\":\"" + e.msg + "\"}"))
        case \/-(t) => 
          var config = new Properties()
          for ((k, v) <- t.config) {
            config.setProperty(k, v)
          }

          val retentionReads: Reads[Int] = (JsPath \ "retention").read[Int]
          val retentionResult: JsResult[Int] = request.body.validate[Int](retentionReads)
          retentionResult match {
            case s: JsSuccess[Int] => config.setProperty("retention.ms", (s.get * 3600 * 1000).toString)
            case e: JsError =>{ }
          }

          val maxSendReads: Reads[Int] = (JsPath \ "max.message.byte").read[Int]
          val maxSendResult: JsResult[Int] = request.body.validate[Int](maxSendReads)
          maxSendResult match {
            case s: JsSuccess[Int] => config.setProperty("max.message.byte", s.get.toString)
            case e: JsError =>{ }
          }

          kafkaManager.updateTopicConfig((request.body \ "cluster").as[String],
            (request.body \ "topic").as[String],
            config,
            t.configReadVersion).map {
              case -\/(e) => Ok("{\"errcode\":\"Failed\", \"errmsg\":\"" + e.msg + "\"}") 
              case \/-(ok) => Ok("{\"errcode\":\"Ok\", \"errmsg\":\"\"}")
            }
      }
  }

  def deleteTopic(clustername:String, topic:String) = Action.async {request =>
    kafkaManager.deleteTopic(clustername, topic).map {
      case -\/(e) => Ok("{\"errcode\":\"Failed\", \"errmsg\":\"" + e.msg + "\"}") 
      case \/-(ok) => Ok("{\"errcode\":\"Ok\", \"errmsg\":\"\"}")
    }
  }

  def addPartitions = Action.async(parse.tolerantJson) {request =>
    var brokerListFuture: Future[ApiError \/ BrokerListExtended] = kafkaManager.getBrokerList((request.body \ "cluster").as[String])
    var topicIdentityFuture: Future[ApiError \/ TopicIdentity] = kafkaManager.getTopicIdentity((request.body \ "cluster").as[String],
      (request.body \ "topic").as[String])

    for {
      brokerList <- brokerListFuture
      topicIdentity <- topicIdentityFuture
      addTopic <- 
      kafkaManager.addTopicPartitions((request.body \ "cluster").as[String],
        (request.body \ "topic").as[String],
        brokerList.fold(
          err => {
            Nil 
          },
          bl => {
            bl.list.map(b => b.id)
          }),
        (request.body \ "partitions").as[Int],
        topicIdentity.fold(err => 0,
          ti => ti.readVersion)
        )
    } yield {
        addTopic match {
          case -\/(e) => Ok("{\"errcode\":\"Failed\", \"errmsg\":\"" + e.msg + "\"}") 
          case \/-(ok) => Ok("{\"errcode\":\"Ok\", \"errmsg\":\"\"}")
        }
    }
  }

  def setTopicMaxSize = Action.async(parse.tolerantJson) {request =>
    var topicConfigTreeCacheInit = false
    val topicConfigTreeCacheListener = new TreeCacheListener {
      override def childEvent(client: CuratorFramework, event: TreeCacheEvent): Unit = {
        event.getType match {
          case TreeCacheEvent.Type.INITIALIZED =>
            topicConfigTreeCacheInit = true
          case _ =>
            //do nothing
        }
      }
    }

    var result = ""
    var errMsg: String = ""
    val clustername = (request.body \ "cluster").as[String]
    val zk_list = QBusUtil.getZkListByClusterName(clustername)
    if (zk_list != "") {
      val curator: CuratorFramework = CuratorFrameworkFactory.newClient(
        zk_list,
        new BoundedExponentialBackoffRetry(1000, 3000, 3))
      curator.start()
      try {
        curator.setData().forPath("/qbus2/config/" + (request.body \ "topic").as[String], (request.body \ "maxsize").as[String].getBytes(StandardCharsets.UTF_8))
      } catch {
        case e: NoNodeException => {
          try {
            curator.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath("/qbus2/config/" + (request.body \ "topic").as[String], (request.body \ "maxsize").as[String].getBytes(StandardCharsets.UTF_8))
          } catch {
            case e: NodeExistsException =>
              curator.setData().forPath("/qbus2/config/" + (request.body \ "topic").as[String], (request.body \ "maxsize").as[String].getBytes(StandardCharsets.UTF_8))
            case e2: Throwable => errMsg = "Faile write data to zk"
          }
        }
            case e2: Throwable => errMsg = "Faile write data to zk"
      }

      Try(curator.close())

      if (errMsg  != "") {
        result = "{\"errcode\":\"Failed\", \"errmsg\":\"" + errMsg + "\"}"
      } else {
        result = "{\"errcode\":\"Ok\", \"errmsg\":\"\"}"
      }
      } else {
        logger.info("Not found cluster: " + clustername)
        result = "{" + "\"errcode\":\"Failed\", \"errmsg\":\"not found cluster:" + clustername + "\"}"
      }

    scala.concurrent.Future{
      Ok(result)
    }
  }

  def getOverageTopicList(clustername: String) = Action.async { request =>
    var overagetopictreecacheinit = false
    val overageTopicTreeCacheListener = new TreeCacheListener {
      override def childEvent(client: CuratorFramework, event: TreeCacheEvent): Unit = {
        event.getType match {
          case TreeCacheEvent.Type.INITIALIZED =>
            overagetopictreecacheinit = true
          case _ =>
            //do nothing
        }
      }
    }

    var result = ""
    val zk_list = QBusUtil.getZkListByClusterName(clustername)
    if (zk_list != "") {
    val curator: CuratorFramework = CuratorFrameworkFactory.newClient(
      zk_list,
      new BoundedExponentialBackoffRetry(1000, 3000, 3))
    curator.start()
    val overageTopicTreeCache = new TreeCache(curator, "/qbus2/status/excess")
    overageTopicTreeCache.start()
    overageTopicTreeCache.getListenable.addListener(overageTopicTreeCacheListener)

    while (!overagetopictreecacheinit) {
      Thread.sleep(100)
    }

    var data = Option(overageTopicTreeCache.getCurrentData("/qbus2/status/excess")).flatMap(currentData =>
        Option(currentData.getData)).map(asString).getOrElse("")

    Try(overageTopicTreeCache.close())
    Try(curator.close())
    if (data != "") {
      result = "{\"errcode\":\"Ok\", \"errmsg\":\"\", \"result\":" + data + "}"
    } else {
      result = "{\"errcode\":\"Failed\", \"errmsg\":\"" + "not found topic" + "\"}"
    }
    } else {
        logger.info("Not found cluster: " + clustername)
        result = "{" + "\"errcode\":\"Failed\", \"errmsg\":\"not found cluster:" + clustername + "\"}"
    }

    scala.concurrent.Future{
      Ok(result)
    }
  }

  def getConsumerAndProducerList(clustername: String, topic: String) = Action.async { request =>
    var result: String = "{\"errcode\":\"Ok\", \"errmsg\":\"\", \"result\":{"

    var registerclienttreecacheinit = false
    val registerclienttreecachelistener = new TreeCacheListener {
      override def childEvent(client: CuratorFramework, event: TreeCacheEvent): Unit = {
        event.getType match {
          case TreeCacheEvent.Type.INITIALIZED =>
            registerclienttreecacheinit = true
          case _ =>
            //do nothing
        }
      }
    }

    val zk_list = QBusUtil.getZkListByClusterName(clustername)
    if (zk_list != "") {
    val curator: CuratorFramework = CuratorFrameworkFactory.newClient(
      zk_list,
      new BoundedExponentialBackoffRetry(1000, 3000, 3))
    curator.start()
    val registerClientTopicTreeCache = new TreeCache(curator, "/qbus2/client/register")
    registerClientTopicTreeCache.start()
    registerClientTopicTreeCache.getListenable.addListener(registerclienttreecachelistener)

    while (!registerclienttreecacheinit) {
      Thread.sleep(100)
    }

    Option(registerClientTopicTreeCache.getCurrentChildren("/qbus2/client/register/" + topic + "/consumer")).map(machineList => {
      var i = 0;
      result += "\"consumer_list\":["
      for ((k,v) <- machineList) {
        if (i != 0) {
          result += ","
        }
        i += 1
        result += "\"" + k + "\""
      }
      result += "]"
    })

    Option(registerClientTopicTreeCache.getCurrentChildren("/qbus2/client/register/" + topic + "/producer")).map(machineList => {
      var i = 0;
      result += ",\"producer_list\":["
      for ((k,v) <- machineList) {
        if (i != 0) {
          result += ","
        }
        i += 1
        result += "\"" + k + "\""
      }
      result += "]"
    })

    result += "}}"
    Try(registerClientTopicTreeCache.close())
    Try(curator.close())
    } else {
        logger.info("Not found cluster: " + clustername)
        result = "{" + "\"errcode\":\"Failed\", \"errmsg\":\"not found cluster:" + clustername + "\"}"
    }

    scala.concurrent.Future{
      Ok(result)
    }
  }

  def registerProducerAndCosumer = Action.async(parse.tolerantJson) { request =>
    var errMsg: String = ""
    var result = ""
    val clustername = (request.body \ "cluster").as[String]
    var zk_list = QBusUtil.getZkListByClusterName(clustername)
    if (zk_list != "") {
    val curator: CuratorFramework = CuratorFrameworkFactory.newClient(
      zk_list,
      new BoundedExponentialBackoffRetry(1000, 3000, 3))
    curator.start()

    var consumerList = (request.body \ "consumer_list").as[List[String]]
    consumerList.foreach(item =>{
      try {
        curator.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath("/qbus2/client/register/" + (request.body \ "topic").as[String] + "/consumer/" + item)
      } catch {
        case e: Throwable =>
      }
    })

    var producerList = (request.body \ "producer_list").as[List[String]]
    producerList.foreach(item =>{
      try {
        curator.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath("/qbus2/client/register/" + (request.body \ "topic").as[String] + "/producer/" + item)
      } catch {
        case e: Throwable =>
      }
    })

    Try(curator.close())
    result = "{\"errcode\":\"Ok\", \"errmsg\":\"\"}"
    } else {
        logger.info("Not found cluster: " + clustername)
        result = "{" + "\"errcode\":\"Failed\", \"errmsg\":\"not found cluster:" + clustername + "\"}"
    }

    scala.concurrent.Future{
      Ok(result)
    }
  }

  def expandAllTopicsToNewBrokerList(clustername: String) = Action.async { request =>
    var topicListFuture: Future[ApiError \/ TopicList] = kafkaManager.getTopicList(clustername)
    var brokerListFuture: Future[ApiError \/ BrokerListExtended] = kafkaManager.getBrokerList(clustername)
    for {
      topicListD <- topicListFuture
      brokerListD <- brokerListFuture
    } yield {
      var errorMsg: String = ""
      var topicList: IndexedSeq[String] = null 
      var brokerList: IndexedSeq[Int] = null 

      brokerListD match {
        case -\/(e) => errorMsg = e.msg
        case \/-(b) => brokerList = b.list.map(bi => bi.id)
      }

      topicListD match {
        case -\/(e) => errorMsg = e.msg
        case \/-(t) => topicList = t.list
      }

      if (errorMsg != "") {
        //TODO:
      } else {
        logger.info("XXX | topic size: " + topicList.size) 

        topicList = topicList.filter(t => t != "__consumer_offsets")

        for (i <- 0 until topicList.size) {
          var topicIdentityFuture: Future[ApiError \/ TopicIdentity] = kafkaManager.getTopicIdentity(clustername,
            topicList(i))
          for {
            topicIdentityD <- topicIdentityFuture
          } yield {
            var partitionCount: Int = 0
            var usedBrokerList: List[Int] = null
            var readVersion: Int = 0
            var topicName: String = ""

            topicIdentityD match {
              case -\/(e) => errorMsg = e.msg
              case \/-(t) => {
                partitionCount = t.partitions
                usedBrokerList = t.partitionsIdentity.toList.flatMap(t => t._2.isr.map(i => (i,t._2.partNum))).groupBy(_._1).mapValues(_.map(_._2)).keySet.toList
                readVersion = t.readVersion
              }
            }

            if (errorMsg != "") {
              //TODO:
            } else {
              if (usedBrokerList.size < brokerList.size) {
                var expandBroker = brokerList.filter(b => !usedBrokerList.contains(b))
                logger.info("XXX | topic name:" + topicList(i) + " | p: " + partitionCount + " | b: " + expandBroker)
                for {
                  addTopic <- 
                  kafkaManager.addTopicPartitions(clustername,
                    topicList(i),
                    expandBroker,
                    partitionCount * brokerList.size / usedBrokerList.size, 
                    readVersion)
                } yield {
                  addTopic match {
                    case -\/(e) => "" 
                    case \/-(ok) => ""
                  }
                }
              }
            }
          }
        }
      }

      Ok("{\"errcode\":\"Ok\", \"errmsg\":\"\"}")
    }
  }

}
