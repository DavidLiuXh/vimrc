package cluster

import (
	"container/heap"
	"errors"
	"fmt"
	"github.com/gogo/protobuf/proto"
	"github.com/influxdata/influxdb/prometheus/remote"
	"math/rand"
	"net"
	"sort"
	"sync"
	"time"

	"github.com/influxdata/influxdb/cluster/meta11"
	"github.com/influxdata/influxdb/tsdb"
	"github.com/influxdata/influxql"
	"go.uber.org/zap"
)

const (
	metaExecutorWriteTimeout        = 5 * time.Second
	metaExecutorMaxWriteConnections = 10
)

// MetaExecutor executes meta queries on all data nodes.
type MetaExecutor struct {
	mu             sync.RWMutex
	timeout        time.Duration
	pool           *clientPool
	maxConnections int
	Logger         *zap.Logger

	nodeExecutor interface {
		executeOnNode(stmt influxql.Statement, database string, node *meta11.NodeInfo) error
		executeOnNode1(stmt influxql.Statement, nodeID uint64, shardIDs []uint64) ([]*tsdb.TagValues, error)
		executeOnNode2(req *remote.ReadRequest, db string, rp string, nodeID uint64, shardIDs []uint64) (*remote.ReadResponseWrap, error)
	}

	MetaClient interface {
		DataNode(id uint64) (ni *meta11.NodeInfo, err error)
		DataNodes() ([]meta11.NodeInfo, error)
	}
}

// NewMetaExecutor returns a new initialized *MetaExecutor.
func NewMetaExecutor(logger *zap.Logger) *MetaExecutor {
	m := &MetaExecutor{
		timeout:        metaExecutorWriteTimeout,
		pool:           newClientPool(),
		maxConnections: metaExecutorMaxWriteConnections,
		Logger:         logger.With(zap.String("service", "meta-executor")),
	}
	m.nodeExecutor = m

	return m
}

// remoteNodeError wraps an error with context about a node that
// returned the error.
type remoteNodeError struct {
	id  uint64
	err error
}

func (e remoteNodeError) Error() string {
	return fmt.Sprintf("partial success, node %d may be down (%s)", e.id, e.err)
}

// ExecuteStatement executes a single InfluxQL statement on all nodes in the cluster concurrently.
func (m *MetaExecutor) ExecuteStatement(stmt influxql.Statement, database string) error {
	// Get a list of all nodes the query needs to be executed on.
	nodes, err := m.MetaClient.DataNodes()
	if err != nil {
		return err
	} else if len(nodes) < 1 {
		return nil
	}

	// Start a goroutine to execute the statement on each of the remote nodes.
	var wg sync.WaitGroup
	errs := make(chan error, len(nodes))
	for _, node := range nodes {
		//if m.Node.ID == node.ID {
		//	continue // Don't execute statement on ourselves.
		//}

		wg.Add(1)
		go func(node meta11.NodeInfo) {
			defer wg.Done()
			if err := m.nodeExecutor.executeOnNode(stmt, database, &node); err != nil {
				errs <- remoteNodeError{id: node.ID, err: err}
			}
		}(node)
	}

	// Wait on n-1 nodes to execute the statement and respond.
	wg.Wait()

	select {
	case err = <-errs:
		return err
	default:
		return nil
	}
}

// executeOnNode executes a single InfluxQL statement on a single node.
func (m *MetaExecutor) executeOnNode(stmt influxql.Statement, database string, node *meta11.NodeInfo) error {
	// We're executing on a remote node so establish a connection.
	c, err := m.dial(node.ID)
	if err != nil {
		return err
	}

	conn, ok := c.(*pooledConn)
	if !ok {
		panic("wrong connection type in MetaExecutor")
	}
	// Return connection to pool by "closing" it.
	defer conn.Close()

	// Build RPC request.
	var request ExecuteStatementRequest
	request.SetStatement(stmt.String())
	request.SetDatabase(database)

	// Marshal into protocol buffer.
	buf, err := request.MarshalBinary()
	if err != nil {
		return err
	}

	// Send request.
	conn.SetWriteDeadline(time.Now().Add(m.timeout))
	if err := WriteTLV(conn, executeStatementRequestMessage, buf); err != nil {
		conn.MarkUnusable()
		return err
	}

	// Read the response.
	conn.SetReadDeadline(time.Now().Add(m.timeout))
	_, buf, err = ReadTLV(conn)
	if err != nil {
		conn.MarkUnusable()
		return err
	}

	// Unmarshal response.
	var response ExecuteStatementResponse
	if err := response.UnmarshalBinary(buf); err != nil {
		return err
	}

	if response.Code() != 0 {
		return fmt.Errorf("error code %d: %s", response.Code(), response.Message())
	}

	return nil
}

//added by zk
// ExecuteStatement executes a single InfluxQL statement on all nodes in the cluster concurrently.
func (m *MetaExecutor) ExecuteStatement1(stmt influxql.Statement, shards []meta11.ShardInfo) (ret []*tsdb.TagValues) {
	nodes, err := m.MetaClient.DataNodes()
	var retc chan []*tsdb.TagValues
	if shards == nil {
		// Get a list of all nodes the query needs to be executed on.
		if err != nil {
			fmt.Print(err)
			return nil
		} else if len(nodes) < 1 {
			fmt.Print(err)
			return nil
		}

		// Start a goroutine to execute the statement on each of the remote nodes.
		var wg sync.WaitGroup
		//errs := make(chan error, len(nodes)-1)
		retc = make(chan []*tsdb.TagValues, len(nodes))
		for _, node := range nodes {
			//if m.Node.ID == node.ID {
			//	continue // Don't execute statement on ourselves.
			//}

			wg.Add(1)
			go func(node meta11.NodeInfo) {
				defer wg.Done()
				ret, err = m.nodeExecutor.executeOnNode1(stmt, node.ID, nil)
				if err != nil {
					fmt.Println(err)
					retc <- nil
				} else {
					retc <- ret
				}
			}(node)
		}
		// Wait on n-1 nodes to execute the statement and respond.
		wg.Wait()

		//select {
		//case err = <-errs:
		//	return nil, err
		//default:
		//	return nil, nil
		//}
	} else {
		// Map shards to nodes.
		shardIDsByNodeID := make(map[uint64][]uint64)
		for _, si := range shards {
			// Always assign to local node if it has the shard.
			// Otherwise randomly select a remote node.
			var nodeID uint64
			if len(si.Owners) > 0 {
				nodeID = si.Owners[rand.Intn(len(si.Owners))].NodeID
			} else {
				// This should not occur but if the shard has no owners then
				// we don't want this to panic by trying to randomly select a node.
				continue
			}

			// Otherwise assign it to a remote shard randomly.
			shardIDsByNodeID[nodeID] = append(shardIDsByNodeID[nodeID], si.ID)
		}
		var wg sync.WaitGroup
		retc = make(chan []*tsdb.TagValues, len(shardIDsByNodeID))
		for nodeID, shardIDs := range shardIDsByNodeID {
			// Sort shard IDs so we get more predicable execution.
			sort.Sort(uint64Slice(shardIDs))
			//if m.Node.ID == nodeID {
			//        continue // Don't execute statement on ourselves.
			//}
			tmp := make([]uint64, len(shardIDs))
			copy(tmp, shardIDs)

			wg.Add(1)
			go func(nodeID uint64) {
				defer wg.Done()
				ret, err = m.nodeExecutor.executeOnNode1(stmt, nodeID, tmp)
				if err != nil { //还是改为判断ret是否为空吧 似乎存在两个同时不为空的情况
					retc <- nil
				} else {
					retc <- ret
				}
			}(nodeID)
		}
		wg.Wait()
		//	select {
		//	case err = <-errs:
		//		return nil, err
		//	default:
		//		return nil, nil
		//	}

		//	return nil, nil
	}

	rets := make([][]*tsdb.TagValues, 0, len(retc))
	size := 0
	for i := 0; i < cap(rets); i++ {
		tmp := <-retc
		if tmp == nil || len(tmp) <= 0 {
			continue
		}
		rets = append(rets, tmp)
		fmt.Println("tmplen", len(tmp), "retslene", len(rets[len(rets)-1]))
		size += len(rets[len(rets)-1])
	}
	ret = m.mergeMTV(rets, size)
	fmt.Println("rrrrrrrrrrrret", ret, size, cap(rets))
	return ret
}

type TagValuesSlice [][]*tsdb.TagValues

func (a *TagValuesSlice) Len() int           { return len(*a) }
func (a *TagValuesSlice) Swap(i, j int)      { (*a)[i], (*a)[j] = (*a)[j], (*a)[i] }
func (a *TagValuesSlice) Less(i, j int) bool { return (*a)[i][0].Measurement < (*a)[j][0].Measurement }
func (a *TagValuesSlice) Push(x interface{}) { *a = append(*a, x.([]*tsdb.TagValues)) }
func (a *TagValuesSlice) Pop() interface{} {
	old := *a
	n := len(old)
	item := old[n-1]
	*a = old[0 : n-1]
	return item
}

func (m *MetaExecutor) mergeMTV(source TagValuesSlice, size int) []*tsdb.TagValues {
	//todo 判断source是否为空
	heap.Init(&source)
	n := 0
	index := 0
	size1 := 0
	ret := make([]*tsdb.TagValues, 0, size)
	for {
		if source.Len() <= 0 {
			break
		}
		tmp := heap.Pop(&source)
		mms := tmp.([]*tsdb.TagValues)
		//if len(mms) == 0 { continue }
		n = len(ret)
		if n > 0 && ret[n-1].Measurement != mms[0].Measurement {
			if index < n-1 {
				ret1 := m.mergeTV(ret[index:n], size1)
				ret[index] = ret1
				ret = ret[:index+1]
			}
			index = len(ret)
			size1 = len(mms[0].Values)
			ret = append(ret, mms[0])
		} else if len(mms[0].Values) > 0 || n == 0 {
			size1 += len(mms[0].Values)
			ret = append(ret, mms[0])
		}
		mms = mms[1:]
		if len(mms) > 0 {
			heap.Push(&source, mms)
		}
	}
	if index < len(ret)-1 { //n不准确  应该改为 len(ret) - 1
		ret1 := m.mergeTV(ret[index:len(ret)], size1)
		ret[index] = ret1
		ret = ret[:index+1]
	}
	return ret
}

type KeyValuesSlice []*tsdb.TagValues

func (a *KeyValuesSlice) Len() int      { return len(*a) }
func (a *KeyValuesSlice) Swap(i, j int) { (*a)[i], (*a)[j] = (*a)[j], (*a)[i] }
func (a *KeyValuesSlice) Less(i, j int) bool {
	ki, kj := (*a)[i].Values[0].Key, (*a)[j].Values[0].Key
	if ki == kj {
		return (*a)[i].Values[0].Value < (*a)[j].Values[0].Value //todo 对values长度为0的判断
	}
	return ki < kj
}
func (a *KeyValuesSlice) Push(x interface{}) { *a = append(*a, x.(*tsdb.TagValues)) }
func (a *KeyValuesSlice) Pop() interface{} {
	old := *a
	n := len(old)
	item := old[n-1]
	*a = old[0 : n-1]
	return item
}

func (m *MetaExecutor) mergeTV(source KeyValuesSlice, size int) *tsdb.TagValues {
	if size == 0 {
		tmp := []*tsdb.TagValues(source)
		if len(tmp) == 0 {
			fmt.Println("jingranzhendehuichuxian")
		}
		return tmp[0]
	}
	heap.Init(&source)
	ret := tsdb.TagValues{}
	ret.Measurement = source[0].Measurement
	ret.Values = make([]tsdb.KeyValue, 0, size)
	for {
		if source.Len() <= 0 {
			break
		}
		tmp := heap.Pop(&source)
		mm := tmp.(*tsdb.TagValues)
		n := len(ret.Values)
		if n > 0 && ret.Values[n-1] == mm.Values[0] { //todo 增加对values为空的判断
			continue
		}
		if len(mm.Values) <= 0 {
			fmt.Println("sourceeeeeeeeeeeeeee", source)
			fmt.Println("vvvvvvvvvvlues is nil", len(mm.Values))
		}
		ret.Values = append(ret.Values, mm.Values[0])
		mm.Values = mm.Values[1:]
		if len(mm.Values) > 0 {
			heap.Push(&source, mm)
		}
	}
	return &ret
}

// executeOnNode executes a single InfluxQL statement on a single node.
func (m *MetaExecutor) executeOnNode1(stmt influxql.Statement, nodeID uint64, shardIDs []uint64) ([]*tsdb.TagValues, error) {
	// We're executing on a remote node so establish a connection.
	c, err := m.dial(nodeID)
	if err != nil {
		return nil, err
	}

	conn, ok := c.(*pooledConn)
	if !ok {
		panic("wrong connection type in MetaExecutor")
	}
	// Return connection to pool by "closing" it.
	defer conn.Close()

	// Build RPC request.
	var request ExecuteStatement1Request
	request.SetStatement(stmt.String())
	//request.SetDatabase(database)

	if shardIDs != nil {
		request.SetShardIDs(shardIDs)
	}
	// Marshal into protocol buffer.
	buf, err := request.MarshalBinary()
	if err != nil {
		return nil, err
	}

	// Send request.
	conn.SetWriteDeadline(time.Now().Add(m.timeout))
	if err := WriteTLV(conn, executeStatement1RequestMessage, buf); err != nil {
		conn.MarkUnusable()
		fmt.Print("====write=============================\n")
		return nil, err
	}

	// Read the response.
	conn.SetReadDeadline(time.Now().Add(m.timeout))
	_, buf, err = ReadTLV(conn)
	if err != nil {
		fmt.Print("====read=============================\n")
		conn.MarkUnusable()
		return nil, err
	}

	// Unmarshal response.
	var response ExecuteStatement1Response
	if err := response.UnmarshalBinary(buf); err != nil {
		return nil, err
	}

	mms := response.Measurements()
	ret := make([]*tsdb.TagValues, 0, len(mms))
	for _, mm := range mms {
		m := tsdb.TagValues{}
		m.Measurement = *mm.Name
		m.Values = make([]tsdb.KeyValue, 0, len(mm.Tags))
		for _, k := range mm.Tags {
			tmp := tsdb.KeyValue{
				Key: *k.K,
				//Value: *k.V,
			}
			if k.V != nil {
				tmp.Value = *k.V
			}
			m.Values = append(m.Values, tmp)
		}
		ret = append(ret, &m)
	}
	return ret, nil
}

//added by zk
// ExecuteStatement executes a single InfluxQL statement on all nodes in the cluster concurrently.
func (m *MetaExecutor) PromRead(req *remote.ReadRequest, shards []meta11.ShardInfo, db string, rp string) (ret *remote.ReadResponseWrap) {
	var retc chan *remote.ReadResponseWrap
	// Map shards to nodes.
	shardIDsByNodeID := make(map[uint64][]uint64)
	for _, si := range shards {
		// Always assign to local node if it has the shard.
		// Otherwise randomly select a remote node.
		var nodeID uint64
		if len(si.Owners) > 0 {
			nodeID = si.Owners[rand.Intn(len(si.Owners))].NodeID
		} else {
			// This should not occur but if the shard has no owners then
			// we don't want this to panic by trying to randomly select a node.
			continue
		}

		// Otherwise assign it to a remote shard randomly.
		shardIDsByNodeID[nodeID] = append(shardIDsByNodeID[nodeID], si.ID)
	}
	var wg sync.WaitGroup
	retc = make(chan *remote.ReadResponseWrap, len(shardIDsByNodeID))

	for nodeID, shardIDs := range shardIDsByNodeID {
		// Sort shard IDs so we get more predicable execution.
		sort.Sort(uint64Slice(shardIDs))
		//if m.Node.ID == nodeID {
		//        continue // Don't execute statement on ourselves.
		//}
		tmp := make([]uint64, len(shardIDs))
		copy(tmp, shardIDs)

		wg.Add(1)
		go func(nodeID uint64) {
			defer wg.Done()
			ret, err := m.nodeExecutor.executeOnNode2(req, db, rp, nodeID, tmp)
			if err != nil { //还是改为判断ret是否为空吧 似乎存在两个同时不为空的情况
				retc <- nil
			} else {
				retc <- ret
			}
		}(nodeID)
	}
	wg.Wait()
	//	select {
	//	case err = <-errs:
	//		return nil, err
	//	default:
	//		return nil, nil
	//	}

	//	return nil, nil

	rets := make([]*remote.ReadResponseWrap, 0, len(retc))
	var size int
	for i := 0; i < cap(rets); i++ {
		tmp := <-retc
		if tmp == nil || len(tmp.Serieskey) <= 0 {
			continue
		}
		rets = append(rets, tmp)
		size += len(tmp.Serieskey)
	}
	source := retSlice(rets)
	ret = &remote.ReadResponseWrap{
		Serieskey:  make([][]byte, size),
		Timeseries: make([]*remote.TimeSeries, size),
	}
	heap.Init(&source)
	for {
		if source.Len() <= 0 {
			break
		}
		resp := heap.Pop(&source)
		tmp := resp.(*remote.ReadResponseWrap)
		ret.Serieskey = append(ret.Serieskey, tmp.Serieskey[0])
		ret.Timeseries = append(ret.Timeseries, tmp.Timeseries[0])
		tmp.Serieskey = tmp.Serieskey[1:]
		tmp.Timeseries = tmp.Timeseries[1:]
		if len(tmp.Serieskey) > 0 {
			heap.Push(&source, tmp)
		}
	}
	return ret
}

type retSlice []*remote.ReadResponseWrap

func (a *retSlice) Len() int      { return len((*a)) }
func (a *retSlice) Swap(i, j int) { (*a)[i], (*a)[j] = (*a)[j], (*a)[i] }
func (a *retSlice) Less(i, j int) bool {
	return tsdb.CompareSeriesKeys((*a)[i].Serieskey[0], (*a)[j].Serieskey[0]) == -1
}
func (a *retSlice) Push(x interface{}) { *a = append(*a, x.(*remote.ReadResponseWrap)) }
func (a *retSlice) Pop() interface{} {
	old := *a
	n := len(old)
	item := old[n-1]
	*a = old[0 : n-1]
	return item
}

// executeOnNode executes a single InfluxQL statement on a single node.
func (m *MetaExecutor) executeOnNode2(req *remote.ReadRequest, db string, rp string, nodeID uint64, shardIDs []uint64) (*remote.ReadResponseWrap, error) {
	// We're executing on a remote node so establish a connection.
	c, err := m.dial(nodeID)
	if err != nil {
		return nil, err
	}

	conn, ok := c.(*pooledConn)
	if !ok {
		panic("wrong connection type in MetaExecutor")
	}
	// Return connection to pool by "closing" it.
	defer conn.Close()

	// Build RPC request.
	request := &remote.ReadRequestWrap{
		Db:       db,
		Rp:       rp,
		Shardids: shardIDs,
		Request:  req,
	}
	// Marshal into protocol buffer.
	buf, err := proto.Marshal(request)
	if err != nil {
		return nil, err
	}

	// Send request.
	conn.SetWriteDeadline(time.Now().Add(m.timeout))
	if err := WriteTLV(conn, promReadRequestMessage, buf); err != nil {
		conn.MarkUnusable()
		fmt.Print("====write=============================\n")
		return nil, err
	}

	// Read the response.
	conn.SetReadDeadline(time.Now().Add(60 * time.Minute))
	_, buf, err = ReadTLV(conn)
	if err != nil {
		fmt.Print("====read=============================\n")
		conn.MarkUnusable()
		return nil, err
	}
	// Unmarshal response.
	response := &remote.ReadResponseWrap{}
	if err := proto.Unmarshal(buf, response); err != nil {
		return nil, err
	}
	if len(response.Timeseries) != len(response.Serieskey) {
		fmt.Println("not equl", len(response.Timeseries), len(response.Serieskey))
		return nil, errors.New("the count of timeseies and serieskey is not equal")
	}
	return response, nil
}

// dial returns a connection to a single node in the cluster.
func (m *MetaExecutor) dial(nodeID uint64) (net.Conn, error) {
	// If we don't have a connection pool for that addr yet, create one
	_, ok := m.pool.getPool(nodeID)
	if !ok {
		factory := &connFactory{nodeID: nodeID, clientPool: m.pool, timeout: m.timeout}
		factory.metaClient = m.MetaClient

		p, err := NewBoundedPool(1, m.maxConnections, m.timeout, factory.dial)
		if err != nil {
			return nil, err
		}
		m.pool.setPool(nodeID, p)
	}
	return m.pool.conn(nodeID)
}
